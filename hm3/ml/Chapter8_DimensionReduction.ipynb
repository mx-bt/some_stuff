{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Top reason to lower the dimensions of a dataset\\n    To make the data correlations understandable,visualize them and fasten up the training process of our ML model.\\n    Further it is empirically proven that most higher dimensional data is very close to a certain manifold within\\n    this data and therefore suitable to be reduced to a lower dimension without losing too much information.\\n\\n2. What is the curse of dimensionality?\\n    Multiple dimensions will vasten the distances between the datapoints and therefore lower the information density.\\n    To tackle this we have two options: a) increase the amount of datapoints, which would result in an unbelievably\\n    big dataset even when trying to make up for 100 dimensions and b) decrease the dimensions.\\n\\n3. Once dimensionality of a dataset is reduced, can this be reversed?\\n    Yes and no. Since the compression always results in a loss of information the original dataset cannot be restored\\n    completely. But if we do it right, the ratio of compression is much greater than the ration of information loss\\n    and therefore, e.g. from a by 25% compressed dataset that holds 95% of variance of the original one, we can decompress\\n    a dataset that is just 5% worse than the original dataset.\\n\\n4. Is PCA suitable to reduce the dimensions of a large non-linear dataset?\\n\\n5. Apply a PCA to a 1000D dataset and fix the ratio of blur to 0.95. How many dimensions remain?\\n\\n6. In which case is the following model ideal?\\n    pure PCA:\\n    incremental PCA:\\n    randomized PCA:\\n    Kernel PCA:\\n\\n7. How can one determine the performance of a dimension reduction algorithm to a dataset?\\n\\n8. Does it make sense to apply two PCA algorithms in a row?\\n\\n'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. Top reason to lower the dimensions of a dataset\n",
    "    To make the data correlations understandable,visualize them and fasten up the training process of our ML model.\n",
    "    Further it is empirically proven that most higher dimensional data is very close to a certain manifold within\n",
    "    this data and therefore suitable to be reduced to a lower dimension without losing too much information.\n",
    "\n",
    "2. What is the curse of dimensionality?\n",
    "    Multiple dimensions will vasten the distances between the datapoints and therefore lower the information density.\n",
    "    To tackle this we have two options: a) increase the amount of datapoints, which would result in an unbelievably\n",
    "    big dataset even when trying to make up for 100 dimensions and b) decrease the dimensions.\n",
    "\n",
    "3. Once dimensionality of a dataset is reduced, can this be reversed?\n",
    "    Yes and no. Since the compression always results in a loss of information the original dataset cannot be restored\n",
    "    completely. But if we do it right, the ratio of compression is much greater than the ration of information loss\n",
    "    and therefore, e.g. from a by 25% compressed dataset that holds 95% of variance of the original one, we can decompress\n",
    "    a dataset that is just 5% worse than the original dataset.\n",
    "\n",
    "4. Is PCA suitable to reduce the dimensions of a large non-linear dataset?\n",
    "    In general PCA scales linear with the size of the dataset but to calculate non-linear datasets we must use a kernelPCA.\n",
    "    Short answer: yes.\n",
    "\n",
    "5. Apply a PCA to a 1000D dataset and fix the ratio of blur to 0.95. How many dimensions remain?\n",
    "    This depends on the information density and variance of the datasets dimensions. The amount of remaining dimensions d\n",
    "    solely depends on the dataset.\n",
    "\n",
    "6. In which case is the following model ideal?\n",
    "    pure PCA: large linear datasets where m roughly equals d\n",
    "    incremental PCA: very large linear datasets where we need to apply the transformation on smaller batches, this is especially\n",
    "    interesting for online applications\n",
    "    randomized PCA: datasets where d << m because it scales better with large m\n",
    "    Kernel PCA: non-linear datasets\n",
    "\n",
    "7. How can one determine the performance of a dimension reduction algorithm to a dataset?\n",
    "    We cannot directly estimate the performance but we can measure the performance of our estimator on the reduced\n",
    "    dataset. Therefore we can choose which hyperparameters of our dimension reduction are the best considering our\n",
    "    dataset and the ML model we want to use.\n",
    "\n",
    "8. Does it make sense to apply two PCA algorithms in a row?\n",
    "    This depends on our goal. Given a 0.95 variance threshold and suppose our first algorithms was executed propertly,\n",
    "    we cannot further reduce the dataset by another PCA because we already reduced everything possible. If we want \n",
    "    to further increase computation speed and tradeoff some variance we can apply another algorithm. Another reason\n",
    "    would be to visualize the pre-compressed dataset in an even lower dimension (1,2 or 3).\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_swiss_roll\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import KernelPCA\n",
    "\n",
    "S, color = make_swiss_roll(n_samples=1000,noise=0.1)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "pca = KernelPCA(n_components=2,kernel=\"rbf\",gamma=0.04)\n",
    "s2d = pca.fit_transform(S)\n",
    "\n",
    "ax.scatter(S[:, 0], S[:, 1], S[:, 2], c=color, cmap=plt.cm.Spectral)\n",
    "ax.scatter(s2d[:, 0], s2d[:, 1], c=color, marker='s',cmap=plt.cm.Spectral)\n",
    "# Set labels and title\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.set_title('Swiss Roll')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfig = plt.figure()\\nax = fig.add_subplot(111, projection='3d')\\nax.scatter(X[:, 0], X[:, 1], X[:, 2], c='b', marker='o')\\nax.set_xlabel('X')\\nax.set_ylabel('Y')\\nax.set_zlabel('Z')\\nax.set_title('Data Visualization')\\n\\n# Show the plot\\n%matplotlib inline\\nplt.show()\""
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(4)\n",
    "m = 60\n",
    "w1, w2 = 0.1, 0.3\n",
    "noise = 0.1\n",
    "\n",
    "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "X = np.empty((m, 3))\n",
    "X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
    "X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
    "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)\n",
    "\n",
    "\"\"\"\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c='b', marker='o')\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.set_title('Data Visualization')\n",
    "\n",
    "# Show the plot\n",
    "%matplotlib inline\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Z')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "%matplotlib qt\n",
    "pca = PCA(n_components=2)\n",
    "x2d = pca.fit_transform(X)\n",
    "x2d\n",
    "X\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:,2], c='b', marker='o')\n",
    "ax.scatter(X[:, 0], X[:, 1], X[:,2], c='b', marker='v')\n",
    "ax.scatter(x2d[:, 0], x2d[:, 1], c='r', marker='s')\n",
    "\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 69\n",
    "n = 1000\n",
    "\n",
    "w1, w2 = 0.1, 0.3\n",
    "noise = 0.1\n",
    "\n",
    "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
    "d1000 = np.empty((m, n))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(d1000)\n",
    "d1000.shape[0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
