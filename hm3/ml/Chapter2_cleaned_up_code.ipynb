{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b350f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import urllib\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml2/master/\" # a download-only linked\n",
    "HOUSING_PATH = os.path.join(\"datasets\",\"housing\") # Defines a path in the folder where I'm currently working in\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\" # defining the saving and extraction path\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH): # defining function to extract and save housing data\n",
    "    os.makedirs(housing_path, exist_ok=True) # # Creates a path in the folder where I'm currently working in, based on housing_path defintion\n",
    "    tgz_path = os.path.join(housing_path,\"housing.tgz\") # Defines a path for the tgz file\n",
    "    urllib.request.urlretrieve(housing_url,tgz_path) # Defines the path from where to where the following actions will be executed\n",
    "    housing_tgz = tarfile.open(tgz_path) # Opens the tarfile in githubusercontent\n",
    "    housing_tgz.extractall(path=housing_path) # Extracts the content of tgz to our housing_path\n",
    "    housing_tgz.close() # closing the tarfile\n",
    "\n",
    "fetch_housing_data() # fetches our housing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563b4082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # Delivers a panda-DataFrame-Object with all data\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path,\"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "housing = load_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97af6255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# housing[\"median_income\"].value_counts(bins=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa005a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_wID = housing.reset_index()\n",
    "housing_wID[\"id\"] = housing[\"latitude\"]*1000 + housing[\"latitude\"] # Intented to create unique IDs for datasets, but doesn't work bruh\n",
    "housing.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7467da6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating \"regular\" train-test-split\n",
    "from sklearn.model_selection import train_test_split\n",
    "TEST_SIZE = 0.2 \n",
    "train_set, test_set = train_test_split(housing,test_size=TEST_SIZE,random_state=42)\n",
    "# We'll see in the following that a standard split is not a good choice in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680cd7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Stratify our median incomes into income categories\"\n",
    "import numpy as np\n",
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"], bins=[0.,1.5,3.0,4.5,6., np.inf], labels=[\"1 Crackrat (0-15k$)\",\"2 Homeless (15-30k$)\",\"3 Peasant (30-45k$)\",\"4 Salaryslave (45-60k$)\",\"5 DevelopsAIinforesthideout (>60k$)\"])\n",
    "housing[\"income_cat\"].hist()\n",
    "# Based on our income categories, we apply a SSS to avoid bias in our small test set\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "SS_split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in SS_split.split(housing,housing[\"income_cat\"]):\n",
    "    strat_train_set = housing.loc[train_index]\n",
    "    strat_test_set = housing.loc[test_index]\n",
    "\n",
    "# Overwriting the train-test-split with our stratified data\n",
    "# TEST_SIZE = 0.2 \n",
    "# train_set, test_set = train_test_split(housing,test_size=TEST_SIZE,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540fde8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf7d706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The proportions of the income_cat groups are defined as the following fractions\n",
    "total_value_counts = housing[\"income_cat\"].value_counts() / len(housing)\n",
    "normal_test_value_counts = test_set[\"income_cat\"].value_counts()/ len(test_set)\n",
    "strat_test_value_counts = strat_test_set[\"income_cat\"].value_counts()/ len(strat_test_set)\n",
    "\n",
    "# The absolute errors\n",
    "abs_n_err = (abs((normal_test_value_counts.values)-(total_value_counts.values)))\n",
    "abs_s_err = (abs((strat_test_value_counts.values)-(total_value_counts.values)))\n",
    "\n",
    "# Convert the Series object to a DataFrame with appropriate column names\n",
    "df_comparison = pd.DataFrame({\n",
    "    \"Income Category\": total_value_counts.index,\n",
    "    \"Normal Proportion\": total_value_counts.values,\n",
    "    \"Test Proportion\": normal_test_value_counts.values,\n",
    "    \"Test Proportion Error\": abs_n_err,\n",
    "    \"Strat Test Proportion\": strat_test_value_counts.values,\n",
    "    \"Strat Test Proportion Error\": abs_s_err\n",
    "})\n",
    "print(df_comparison)\n",
    "\n",
    "# After using the stratisfyer category, we shall delete it from our train and test sets to avoid influence on our further procedure\n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop(\"income_cat\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685b3dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_train = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05839c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "housing_train.plot(kind=\"scatter\", x=\"longitude\",y=\"latitude\",alpha=0.5,\n",
    "                 s=housing_train[\"population\"]/100,c=housing_train[\"median_house_value\"],\n",
    "                 cmap=plt.get_cmap(\"jet\"),\n",
    "                 label=\"=population size\",\n",
    "                 colorbar=True\n",
    "                 ) \n",
    "# Plotting the districts with alpha=0.1 (transparency) \n",
    "# s is the radius of each circle, representing the total population of the district\n",
    "# c determines the color of each circle\n",
    "# cmap defines the red to blue scheme\n",
    "# colorbar displays the color-value correlation\n",
    "train_set_elements = str(len(housing_train))\n",
    "plt.title(label=f\"Train set elements: {train_set_elements}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec63bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching for useful correlation in the data\n",
    "corr_matrix = housing_train.corr()\n",
    "print(corr_matrix[\"median_house_value\"].sort_values(ascending=False))\n",
    "# Create a heatmap using the DataFrame\n",
    "import seaborn as sns\n",
    "# create heatmap using Seaborn\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37b4f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix as ppsm\n",
    "attributes = [\"median_house_value\",\"median_income\",\"total_rooms\",\"housing_median_age\"]\n",
    "ppsm(housing_train[attributes],figsize=(12,8),alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bdd5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Adding some custom properties for more noteworthy metrics\"\"\"\n",
    "housing_train[\"rooms_per_household\"]=housing_train[\"total_rooms\"]/housing_train[\"households\"]\n",
    "housing_train[\"bedrooms_per_room\"]=housing_train[\"total_bedrooms\"]/housing_train[\"total_rooms\"]\n",
    "housing_train[\"population_per_household\"]=housing_train[\"population\"]/housing_train[\"households\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a698cb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Checking out some correlations\"\"\"\n",
    "print(corr_matrix[\"median_house_value\"].sort_values(ascending=False))\n",
    "corr_matrixnew = housing_train.corr()\n",
    "print(corr_matrixnew[\"median_house_value\"].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576d69de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the labels from the train set\n",
    "trainset_w_features = strat_train_set.drop(\"median_house_value\",axis=1)\n",
    "trainset_w_label = strat_train_set[\"median_house_value\"].copy()\n",
    "# Creating numerical trainset for our Simple Imputer\n",
    "trainset_num = trainset_w_features.drop(\"ocean_proximity\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f128fbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_w_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54efc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "imputer.fit(trainset_num) # Fitting the Imputer instance to the data set\n",
    "\n",
    "XX = imputer.transform(trainset_num)\n",
    "trainset_num_tr = pd.DataFrame(XX, columns=trainset_num.columns, index=trainset_num.index)\n",
    "\n",
    "#X = imputer.transform(housing_num)\n",
    "#housing_tr = pd.DataFrame(X, columns=housing_num.columns, index=housing_num.index)\n",
    "#housing_tr.info()\n",
    "#housing_num.info()\n",
    "\n",
    "trainset_opcat = trainset_w_features[[\"ocean_proximity\"]]\n",
    "#trainset_opcat.reshape(-1, 1) # if your data has a single feature or\n",
    "#print(trainset_opcat) \n",
    "#trainset_opcat.reshape(1, -1)\n",
    "print(trainset_opcat) \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "OHE = OneHotEncoder()\n",
    "housing_cat_1hot = OHE.fit_transform(trainset_opcat)\n",
    "print(housing_cat_1hot.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f729bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(type(trainset_w_features[[\"ocean_proximity\"]]))\n",
    "print(type(trainset_w_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224a6515",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(trainset_w_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf3e82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Definition of Own Transformer\"\"\"\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "rooms_ix, bedrooms_ix, population_ix, households_ix = 3,4,5,6\n",
    "class CombinedAttributesAdder(BaseEstimator,TransformerMixin):\n",
    "    def __init__(self,add_bedrooms_per_room = True):\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    def transform(self,X,y=None):\n",
    "        rooms_per_household = X[:,rooms_ix]/X[:,households_ix]\n",
    "        population_per_household = X[:,population_ix]/X[:,households_ix]\n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:,bedrooms_ix]/X[:,rooms_ix]\n",
    "            return np.c_[X,rooms_per_household,population_per_household,bedrooms_per_room]\n",
    "        else:\n",
    "            return np.c_[X,rooms_per_household,population_per_household]\n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs = attr_adder.transform(housing.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c78c8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for automatic data transformation\n",
    "# Output is basically all we did before, but faster\n",
    "# Tranforms all numerical values into the Standard-Derivation form\n",
    "# Tranforms all categorical values into a OneHot-Matrix\n",
    "# type(housing_prepared) = numpy.ndarray !\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"attribs_adder\", CombinedAttributesAdder()),\n",
    "    (\"std_scaler\",StandardScaler())    \n",
    "])\n",
    "housing_num_tr = num_pipeline.fit_transform(trainset_num)\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "num_attribs = list(trainset_num)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\",num_pipeline,num_attribs),\n",
    "    (\"cat\", OneHotEncoder(),cat_attribs)\n",
    "])\n",
    "housing_prepared = full_pipeline.fit_transform(trainset_w_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b203ed2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae4aaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Model Training and Testing on TRAINING DATA\"\"\"\n",
    "\"\"\"No1 - Linear Regression\"\"\"\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(housing_prepared, trainset_w_label)\n",
    "some_data = trainset_w_features.iloc[:5]\n",
    "some_labels = trainset_w_label.iloc[:5]\n",
    "some_data_prepared = full_pipeline.transform(some_data) # Applies the \"fast tranform\" to our data\n",
    "rndd_prdctn = [round(num, 0) for num in lin_reg.predict(some_data_prepared)]\n",
    "\n",
    "# For big data sets, we should test our code with a smaller batch of the train data\n",
    "print(\"===For 5 Datapoints===\")\n",
    "print(\"Prediction for 5 data points: \", rndd_prdctn)\n",
    "print(\"True Values for 5 data points: \", list(some_labels))\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "lin_reg_mse = np.sqrt(mse(rndd_prdctn, some_labels))\n",
    "print(\"MSE for 5 data points: \", lin_reg_mse)\n",
    "\n",
    "# Now the whole dataset w/ Linear Regression\n",
    "full_rndd_prdctn = [round(pred,0) for pred in lin_reg.predict(housing_prepared)]\n",
    "print(\"===For the Whole Set===\")\n",
    "print(\"Prediction for whole set: \", full_rndd_prdctn)\n",
    "print(\"True Values for whole set: \", list(trainset_w_label))\n",
    "full_lin_reg_mse = np.sqrt(mse(full_rndd_prdctn, trainset_w_label))\n",
    "print(\"MSE for whole set: \", full_lin_reg_mse)\n",
    "print(\"===Summary===\")\n",
    "print(\"Relative difference of mean error and average housing price: \", round((full_lin_reg_mse*100/trainset_w_label.mean()),1), \"%\")\n",
    "# The current error is 33.2 % of the average house price, this is nuts, and too much for a good prediction\n",
    "# Experts are usually capable of predicting the house price with 20% accuracy, so we should work towards that\n",
    "\n",
    "\"\"\"Cross Validation of the LinReg Century Error\"\"\"\n",
    "from sklearn.model_selection import cross_val_score as cvs\n",
    "scores2 = cvs(lin_reg,housing_prepared,trainset_w_label,scoring=\"neg_mean_squared_error\", cv=10)\n",
    "print(list(int(round(num,0)) for num in np.sqrt(-scores2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d2b0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"No2 - Decision Tree\"\"\"\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dtr = DecisionTreeRegressor()\n",
    "dtr.fit(housing_prepared, trainset_w_label)\n",
    "dtr.predict(housing_prepared)\n",
    "print(\"MSE for the DTR model: \",np.sqrt(mse(dtr.predict(housing_prepared),trainset_w_label)))\n",
    "# MSE for the DTR model:  0.0\n",
    "# This is a sign of overfitting hardcore\n",
    "\n",
    "\"\"\"Cross Validation of the DTR Fraud\"\"\"\n",
    "# Scikit cross_val_score separates our (training) data into chunks for in-between fit check\n",
    "\n",
    "tree_scores = cvs(dtr,housing_prepared,trainset_w_label,scoring=\"neg_mean_squared_error\", cv=10) \n",
    "tree_rmse_scores = np.sqrt(-tree_scores)\n",
    "\n",
    "def display_scores(scores):\n",
    "    print(\"Scores: \", scores)\n",
    "    print(\"Mean: \", scores.mean())\n",
    "    print(\"Standard Derivation: \",(scores.std()))\n",
    "\n",
    "display_scores(tree_rmse_scores)\n",
    "# [72628, 71478, 68834, 72742, 69408, 76213, 69743, 73693, 68960, 67938]\n",
    "# As we see, this is even worse than our linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc41eb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"No3 - Random Forest Regressor\"\"\"\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfr = RandomForestRegressor()\n",
    "rfr.fit(housing_prepared, trainset_w_label)\n",
    "print(\"Checkpoint: Model fitting done\")\n",
    "rfr_predict = rfr.predict(housing_prepared)\n",
    "print(\"Checkpoint: Predictions done\")\n",
    "\n",
    "forest_scores = cvs(rfr,housing_prepared,trainset_w_label,scoring=\"neg_mean_squared_error\", cv=10) \n",
    "forest_rmse_scores = np.sqrt(-forest_scores)\n",
    "\n",
    "display_scores(forest_rmse_scores)\n",
    "print(\"MSE Calc: \", np.sqrt(mse(rfr_predict,trainset_w_label)))\n",
    "# took over 3 minutes\n",
    "# The RFR is the best we have even though the error is quite high\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802b2f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Model Optimization\"\"\"\n",
    "\"\"\"Systematic Search For Hyperparameters: Grid Search\"\"\"\n",
    "from sklearn.model_selection import GridSearchCV as GS\n",
    "param_grid = [\n",
    "    {\"n_estimators\":[10,30],\"max_features\":[8,12]},\n",
    "    {\"bootstrap\":[False],\"n_estimators\":[10,30],\"max_features\":[8,12]}\n",
    "]\n",
    "\n",
    "grid_search = GS(rfr,\n",
    "                 param_grid,\n",
    "                 cv=3,\n",
    "                 scoring=\"neg_mean_squared_error\",\n",
    "                 return_train_score=True\n",
    ")\n",
    "\n",
    "grid_search.fit(housing_prepared,trainset_w_label) # Takes a while >3min\n",
    "best_parameters = grid_search.best_params_\n",
    "print(\"Best Par: \", best_parameters)\n",
    "best_estimator = grid_search.best_estimator_\n",
    "print(\"Best Est: \",best_estimator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e8ac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Grid Search Results\"\"\"\n",
    "cvres = grid_search.cv_results_\n",
    "for mean_score, params in zip (cvres[\"mean_test_score\"],cvres[\"params\"]):\n",
    "    print(np.sqrt(-mean_score),params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb87954",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Feature Importance\"\"\"\n",
    "feature_importances = grid_search.best_estimator_.feature_importances_\n",
    "print(\"Default : \", feature_importances)\n",
    "extra_attribs = [\"rooms_per_hhld\",\"population_per_hhld\",\"bedrooms_per_room\"]\n",
    "cat_encoder1 = full_pipeline.named_transformers_[\"cat\"]\n",
    "cat_1hot_attribs = list(cat_encoder1.categories_[0])\n",
    "attributes1 = num_attribs + extra_attribs + cat_1hot_attribs\n",
    "print(\"\")\n",
    "\n",
    "resultz = sorted(zip(feature_importances, attributes1), reverse=True)\n",
    "for importance, attribute in resultz:\n",
    "    print(f\"Importance: {importance}, Attribute: {attribute}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb78ccb3",
   "metadata": {},
   "source": [
    "# We can probably drop population, total_rooms, total_bedrooms, households, and ISLAND, NEAR BAY,NEAR OCEAN,<1H OCEAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56503f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluation on Test Data\"\"\"\n",
    "final_model = grid_search.best_estimator_\n",
    "print(\"Model used: \", final_model)\n",
    "\n",
    "X_test = strat_test_set.drop(\"median_house_value\",axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "\n",
    "y_pred = pd.Series(final_model.predict(X_test_prepared))\n",
    "\n",
    "final_rmse = np.sqrt(mse(y_test, y_pred))\n",
    "print(\"RMSE :\",final_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ecdd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "confidence = 0.95\n",
    "y_pred1 = y_pred.reset_index(drop=True)\n",
    "y_test1 = y_test.reset_index(drop=True)\n",
    "squared_errors = (y_pred1 - y_test1) ** 2\n",
    "interval = np.sqrt(stats.t.interval(confidence,len(squared_errors)-1,\n",
    "                         loc=squared_errors.mean(),\n",
    "                         scale=stats.sem(squared_errors)))\n",
    "\n",
    "\"The confidence level specified in the function is 0.95,\"\n",
    "print(f\"There is a 95% probability that the true mean squared error is between {round(interval[0],0)} and {round(interval[1],0)}.\")\n",
    "interval\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
