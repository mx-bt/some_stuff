{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. Suppose 5 different models reach 0.95 accuracy on a certain dataset. Does it make sense to combine them?\\n    Absolutely. The only requirement that determines if the combination will actually lead to a higher accuracy\\n    is that the models make different mistakes. In this way by implementing hard/soft voting we might have a chance\\n    to even out the mistakes of each other.\\n\\n2. Difference between hard- and soft voting?\\n    Hardvoting: each classifier has the same weight when choosing the datapoint label. The final prediction label\\n    equals the mode of all votes.\\n    Softvoting: given that the classifier can generate a probability (or at least something similar) the final prediction\\n    label is based on a weighted decision, that depends on how sure an estimator is about it\\'s respective prediction.\\n\\n3. Can we parallelize the ensemble training on multiple servers in the following cases?\\na)  Bagging: Yes, since the train data samples are completely randomized we can also access our entire train data with different\\n    servers and make our estimators run simultaneously and speed up the process.\\nb)  Boosting: No, because the iterations depend on the results of the previous iteration. Therefore a split of our operations on\\n    multiple servers would probabl worsen the overall calculation speed.\\nc)  Random Forests: Yes, we could assign a new server for each tree of the forest.\\nd)  Stacking: Yes, we can speed up the process by separating the models of each layer to a different server. The only restriction is,\\n    that we have to wait for each server to completely finish his estimation before continueing to the next layer.\\n\\n4. Benefit of OOB Evaluation?\\n    We don\\'t need to assign a test data set, since we can use the remainder of our Bagging ensemble and use it as some type of \"pseudo\\n    test set\". This should only work correctly if the relative amount of all possible labels is somehow equal and the dataset is fairly\\n    large.\\n\\n5. Why are Extra-Trees more random than regular RandomForests?\\n    The key difference lays in the feature benchmark calculation. While regular RandomForest indiviually calculate the significance bench-\\n    mark of the best features of their dataset, Extra-Trees randomly assign the benchmark and therefore include a random number of\\n    features. This makes the significantly faster than regular random forest since the assignment of suitable features grasps a good share\\n    of the total calculation complexity. Extra trees trade off an even higher bias for lower variance (like all ensembles basically).\\n\\n6. What hyperparameter adjustments to treat an underfitting AdaBoost ensemble?\\n    AdaBoost can underfit the data if there are too few iterations. First, we should try to increase the number of\\n    iterations to a level the calculation effort is not increasing too much. After that we might also play with the learning rate, which \\n    is usually too low. If there are regularization parameters involved it\\'s worth to check and probably decrease their impact.\\n\\n7. How to adjust the learning rate of an overfitting Gradient-Boosting ensemble?\\n    We should decrease the learning rate and lower the number of iterations.\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. Suppose 5 different models reach 0.95 accuracy on a certain dataset. Does it make sense to combine them?\n",
    "    Absolutely. The only requirement that determines if the combination will actually lead to a higher accuracy\n",
    "    is that the models make different mistakes. In this way by implementing hard/soft voting we might have a chance\n",
    "    to even out the mistakes of each other.\n",
    "\n",
    "2. Difference between hard- and soft voting?\n",
    "    Hardvoting: each classifier has the same weight when choosing the datapoint label. The final prediction label\n",
    "    equals the mode of all votes.\n",
    "    Softvoting: given that the classifier can generate a probability (or at least something similar) the final prediction\n",
    "    label is based on a weighted decision, that depends on how sure an estimator is about it's respective prediction.\n",
    "\n",
    "3. Can we parallelize the ensemble training on multiple servers in the following cases?\n",
    "a)  Bagging: Yes, since the train data samples are completely randomized we can also access our entire train data with different\n",
    "    servers and make our estimators run simultaneously and speed up the process.\n",
    "b)  Boosting: No, because the iterations depend on the results of the previous iteration. Therefore a split of our operations on\n",
    "    multiple servers would probabl worsen the overall calculation speed.\n",
    "c)  Random Forests: Yes, we could assign a new server for each tree of the forest.\n",
    "d)  Stacking: Yes, we can speed up the process by separating the models of each layer to a different server. The only restriction is,\n",
    "    that we have to wait for each server to completely finish his estimation before continueing to the next layer.\n",
    "\n",
    "4. Benefit of OOB Evaluation?\n",
    "    We don't need to assign a test data set, since we can use the remainder of our Bagging ensemble and use it as some type of \"pseudo\n",
    "    test set\". This should only work correctly if the relative amount of all possible labels is somehow equal and the dataset is fairly\n",
    "    large.\n",
    "\n",
    "5. Why are Extra-Trees more random than regular RandomForests?\n",
    "    The key difference lays in the feature benchmark calculation. While regular RandomForest indiviually calculate the significance bench-\n",
    "    mark of the best features of their dataset, Extra-Trees randomly assign the benchmark and therefore include a random number of\n",
    "    features. This makes the significantly faster than regular random forest since the assignment of suitable features grasps a good share\n",
    "    of the total calculation complexity. Extra trees trade off an even higher bias for lower variance (like all ensembles basically).\n",
    "\n",
    "6. What hyperparameter adjustments to treat an underfitting AdaBoost ensemble?\n",
    "    AdaBoost can underfit the data if there are too few iterations. First, we should try to increase the number of\n",
    "    iterations to a level the calculation effort is not increasing too much. After that we might also play with the learning rate, which \n",
    "    is usually too low. If there are regularization parameters involved it's worth to check and probably decrease their impact.\n",
    "\n",
    "7. How to adjust the learning rate of an overfitting Gradient-Boosting ensemble?\n",
    "    We should decrease the learning rate and lower the number of iterations.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n8. Applying Ensemble to MNIST\\n\\n- train RandomForest, ExtraTrees + SVM on MNIST\\n- try to combine them to an ensemble with hard and/or soft voting\\n\\nPrevious scores on MNIST\\n========================\\nKNN MNIST data: 0.9714\\nKNN with augmented MNIST data: 0.97815\\nSVM Classifier: {\\'svc__estimator__C\\': 0.07, \\'svc__estimator__gamma\\': 0.1, \\'svc__estimator__kernel\\': \\'poly\\'}: 0.9519\\n\\nTO DO:\\n\"import\" our augmented data creater\\n\"import\" our SVM Classifier\\n\"import\" our KNN classifier\\n\\n- create RandomForest and ExtraTrees \\n\\n- hard/soft vote the results with VotingClassifier \\n    or\\n- create Stacking engine based on the results of 3 classifiers\\n\\n\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "8. Applying Ensemble to MNIST\n",
    "\n",
    "- train RandomForest, ExtraTrees + SVM on MNIST\n",
    "- try to combine them to an ensemble with hard and/or soft voting\n",
    "\n",
    "Previous scores on MNIST\n",
    "========================\n",
    "KNN MNIST data: 0.9714\n",
    "KNN with augmented MNIST data: 0.97815\n",
    "SVM Classifier: {'svc__estimator__C': 0.07, 'svc__estimator__gamma': 0.1, 'svc__estimator__kernel': 'poly'}: 0.9519\n",
    "\n",
    "TO DO:\n",
    "\"import\" our augmented data creater\n",
    "\"import\" our SVM Classifier\n",
    "\"import\" our KNN classifier\n",
    "\n",
    "- create RandomForest and ExtraTrees \n",
    "\n",
    "- hard/soft vote the results with VotingClassifier \n",
    "    or\n",
    "- create Stacking engine based on the results of 3 classifiers\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "50000\n",
      "10000\n",
      "10000\n",
      "10000\n",
      "10000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Normal Data Proportion</th>\n",
       "      <th>Validation Set Proportion</th>\n",
       "      <th>Train Set Proportion</th>\n",
       "      <th>Test Set Proportion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.112529</td>\n",
       "      <td>0.1124</td>\n",
       "      <td>0.11236</td>\n",
       "      <td>0.1135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.104186</td>\n",
       "      <td>0.1044</td>\n",
       "      <td>0.10442</td>\n",
       "      <td>0.1032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.102014</td>\n",
       "      <td>0.1022</td>\n",
       "      <td>0.10218</td>\n",
       "      <td>0.1028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.099857</td>\n",
       "      <td>0.0993</td>\n",
       "      <td>0.09930</td>\n",
       "      <td>0.1010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.099400</td>\n",
       "      <td>0.0992</td>\n",
       "      <td>0.09914</td>\n",
       "      <td>0.1009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.098614</td>\n",
       "      <td>0.0987</td>\n",
       "      <td>0.09872</td>\n",
       "      <td>0.0982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.098229</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.09864</td>\n",
       "      <td>0.0980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.097500</td>\n",
       "      <td>0.0975</td>\n",
       "      <td>0.09752</td>\n",
       "      <td>0.0974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.097486</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.09736</td>\n",
       "      <td>0.0958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.090186</td>\n",
       "      <td>0.0903</td>\n",
       "      <td>0.09036</td>\n",
       "      <td>0.0892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Normal Data Proportion  Validation Set Proportion  Train Set Proportion  \\\n",
       "0                0.112529                     0.1124               0.11236   \n",
       "1                0.104186                     0.1044               0.10442   \n",
       "2                0.102014                     0.1022               0.10218   \n",
       "3                0.099857                     0.0993               0.09930   \n",
       "4                0.099400                     0.0992               0.09914   \n",
       "5                0.098614                     0.0987               0.09872   \n",
       "6                0.098229                     0.0986               0.09864   \n",
       "7                0.097500                     0.0975               0.09752   \n",
       "8                0.097486                     0.0974               0.09736   \n",
       "9                0.090186                     0.0903               0.09036   \n",
       "\n",
       "   Test Set Proportion  \n",
       "0               0.1135  \n",
       "1               0.1032  \n",
       "2               0.1028  \n",
       "3               0.1010  \n",
       "4               0.1009  \n",
       "5               0.0982  \n",
       "6               0.0980  \n",
       "7               0.0974  \n",
       "8               0.0958  \n",
       "9               0.0892  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Get and (augmentation ONLY after successfull testing) MNIST data\"\"\"\n",
    "from joblib import load\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "mnist_data = load(\"C:/Users/MaxB2/Documents/Machine_Is_Learning/mnist_dataset_784_v1.joblib\")\n",
    "\n",
    "X,y = mnist_data[\"data\"],mnist_data[\"target\"]\n",
    "y = y.astype(np.uint8)\n",
    "X_train, X_test, y_train, y_test = X[:60000],X[60000:],y[:60000],y[60000:] \n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "SS_split = StratifiedShuffleSplit(n_splits=5, test_size=(1/6), random_state=42)\n",
    "for train_index, test_index in SS_split.split(X_train,y_train):\n",
    "    X_train_50k = X_train.loc[train_index]\n",
    "    y_train_50k = y_train.loc[train_index]\n",
    "    X_validate_10k = X_train.loc[test_index]\n",
    "    y_validate_10k = y_train.loc[test_index]\n",
    "\n",
    "print(len(X_train_50k))\n",
    "print(len(y_train_50k))\n",
    "print(len(X_validate_10k))\n",
    "print(len(y_validate_10k))\n",
    "print(len(X_test))\n",
    "print(len(y_test))\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# The proportions of the income_cat groups are defined as the following fractions\n",
    "total_digit_counts = mnist_data[\"target\"].value_counts() / len(mnist_data[\"target\"])\n",
    "strat_validation_value_counts = y_validate_10k.value_counts()/ len(X_validate_10k)\n",
    "train_set_value_counts = y_train_50k.value_counts()/ len(X_train_50k)\n",
    "test_set_value_counts = y_test.value_counts()/ len(X_test)\n",
    "# The absolute value of their respective errors are described as \n",
    "\n",
    "# Convert the Series object to a DataFrame with appropriate column names\n",
    "df_comparison = pd.DataFrame({\n",
    "    \"Normal Data Proportion\": total_digit_counts.values,\n",
    "    \"Validation Set Proportion\": strat_validation_value_counts.values,\n",
    "    \"Train Set Proportion\":train_set_value_counts.values,\n",
    "    \"Test Set Proportion\":test_set_value_counts.values\n",
    "})\n",
    "df_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mnist_data[\"target\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9759"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=4, weights='distance')\n",
    "knn.fit(X_train_50k,y_train_50k)\n",
    "y_val_pred_knn = knn.predict(X_validate_10k)\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc_y_val_pred_knn = accuracy_score(y_validate_10k, y_val_pred_knn)\n",
    "acc_y_val_pred_knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9634"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RFR\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rfr = RandomForestClassifier(n_estimators=200,max_leaf_nodes=2048,n_jobs=-1)\n",
    "rfr.fit(X_train_50k,y_train_50k)\n",
    "y_val_pred_rfr = rfr.predict(X_validate_10k)\n",
    "acc_y_val_pred_rfr = accuracy_score(y_validate_10k, y_val_pred_rfr)\n",
    "acc_y_val_pred_rfr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9603"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "etc = ExtraTreesClassifier(n_estimators=200,max_leaf_nodes=2048,n_jobs=-1)\n",
    "etc.fit(X_train_50k,y_train_50k)\n",
    "y_val_pred_etc = etc.predict(X_validate_10k)\n",
    "acc_y_val_pred_etc = accuracy_score(y_validate_10k, y_val_pred_etc)\n",
    "acc_y_val_pred_etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pixel1      0.0\n",
       "pixel2      0.0\n",
       "pixel3      0.0\n",
       "pixel4      0.0\n",
       "pixel5      0.0\n",
       "           ... \n",
       "pixel780    0.0\n",
       "pixel781    0.0\n",
       "pixel782    0.0\n",
       "pixel783    0.0\n",
       "pixel784    0.0\n",
       "Name: 20781, Length: 784, dtype: float64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_validate_10k.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_validate_10k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(X_train_50k)):\n\u001b[0;32m     18\u001b[0m     \u001b[39mfor\u001b[39;00m estimator \u001b[39min\u001b[39;00m estimators:\n\u001b[1;32m---> 19\u001b[0m         temp_triple\u001b[39m.\u001b[39mappend(estimator\u001b[39m.\u001b[39;49mpredict(X_train_50k\u001b[39m.\u001b[39;49miloc[index:index\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m]))\n\u001b[0;32m     20\u001b[0m     triple_predictions\u001b[39m.\u001b[39mappend([temp_triple])\n\u001b[0;32m     21\u001b[0m     temp_triple \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\MaxB2\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:237\u001b[0m, in \u001b[0;36mKNeighborsClassifier.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    235\u001b[0m     neigh_dist \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 237\u001b[0m     neigh_dist, neigh_ind \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkneighbors(X)\n\u001b[0;32m    239\u001b[0m classes_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\n\u001b[0;32m    240\u001b[0m _y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_y\n",
      "File \u001b[1;32mc:\\Users\\MaxB2\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_base.py:824\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    817\u001b[0m use_pairwise_distances_reductions \u001b[39m=\u001b[39m (\n\u001b[0;32m    818\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbrute\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    819\u001b[0m     \u001b[39mand\u001b[39;00m ArgKmin\u001b[39m.\u001b[39mis_usable_for(\n\u001b[0;32m    820\u001b[0m         X \u001b[39mif\u001b[39;00m X \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meffective_metric_\n\u001b[0;32m    821\u001b[0m     )\n\u001b[0;32m    822\u001b[0m )\n\u001b[0;32m    823\u001b[0m \u001b[39mif\u001b[39;00m use_pairwise_distances_reductions:\n\u001b[1;32m--> 824\u001b[0m     results \u001b[39m=\u001b[39m ArgKmin\u001b[39m.\u001b[39;49mcompute(\n\u001b[0;32m    825\u001b[0m         X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    826\u001b[0m         Y\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_X,\n\u001b[0;32m    827\u001b[0m         k\u001b[39m=\u001b[39;49mn_neighbors,\n\u001b[0;32m    828\u001b[0m         metric\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meffective_metric_,\n\u001b[0;32m    829\u001b[0m         metric_kwargs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meffective_metric_params_,\n\u001b[0;32m    830\u001b[0m         strategy\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mauto\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    831\u001b[0m         return_distance\u001b[39m=\u001b[39;49mreturn_distance,\n\u001b[0;32m    832\u001b[0m     )\n\u001b[0;32m    834\u001b[0m \u001b[39melif\u001b[39;00m (\n\u001b[0;32m    835\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit_method \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbrute\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetric \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mprecomputed\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m issparse(X)\n\u001b[0;32m    836\u001b[0m ):\n\u001b[0;32m    837\u001b[0m     results \u001b[39m=\u001b[39m _kneighbors_from_graph(\n\u001b[0;32m    838\u001b[0m         X, n_neighbors\u001b[39m=\u001b[39mn_neighbors, return_distance\u001b[39m=\u001b[39mreturn_distance\n\u001b[0;32m    839\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\MaxB2\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py:277\u001b[0m, in \u001b[0;36mArgKmin.compute\u001b[1;34m(cls, X, Y, k, metric, chunk_size, metric_kwargs, strategy, return_distance)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[39m\"\"\"Compute the argkmin reduction.\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \n\u001b[0;32m    198\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[39mreturns.\u001b[39;00m\n\u001b[0;32m    275\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[39mif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m Y\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mfloat64:\n\u001b[1;32m--> 277\u001b[0m     \u001b[39mreturn\u001b[39;00m ArgKmin64\u001b[39m.\u001b[39;49mcompute(\n\u001b[0;32m    278\u001b[0m         X\u001b[39m=\u001b[39;49mX,\n\u001b[0;32m    279\u001b[0m         Y\u001b[39m=\u001b[39;49mY,\n\u001b[0;32m    280\u001b[0m         k\u001b[39m=\u001b[39;49mk,\n\u001b[0;32m    281\u001b[0m         metric\u001b[39m=\u001b[39;49mmetric,\n\u001b[0;32m    282\u001b[0m         chunk_size\u001b[39m=\u001b[39;49mchunk_size,\n\u001b[0;32m    283\u001b[0m         metric_kwargs\u001b[39m=\u001b[39;49mmetric_kwargs,\n\u001b[0;32m    284\u001b[0m         strategy\u001b[39m=\u001b[39;49mstrategy,\n\u001b[0;32m    285\u001b[0m         return_distance\u001b[39m=\u001b[39;49mreturn_distance,\n\u001b[0;32m    286\u001b[0m     )\n\u001b[0;32m    288\u001b[0m \u001b[39mif\u001b[39;00m X\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m Y\u001b[39m.\u001b[39mdtype \u001b[39m==\u001b[39m np\u001b[39m.\u001b[39mfloat32:\n\u001b[0;32m    289\u001b[0m     \u001b[39mreturn\u001b[39;00m ArgKmin32\u001b[39m.\u001b[39mcompute(\n\u001b[0;32m    290\u001b[0m         X\u001b[39m=\u001b[39mX,\n\u001b[0;32m    291\u001b[0m         Y\u001b[39m=\u001b[39mY,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    297\u001b[0m         return_distance\u001b[39m=\u001b[39mreturn_distance,\n\u001b[0;32m    298\u001b[0m     )\n",
      "File \u001b[1;32msklearn\\metrics\\_pairwise_distances_reduction\\_argkmin.pyx:95\u001b[0m, in \u001b[0;36msklearn.metrics._pairwise_distances_reduction._argkmin.ArgKmin64.compute\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\MaxB2\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py:139\u001b[0m, in \u001b[0;36mthreadpool_limits\u001b[1;34m(limits, user_api)\u001b[0m\n\u001b[0;32m    137\u001b[0m     \u001b[39mreturn\u001b[39;00m controller\u001b[39m.\u001b[39mlimit(limits\u001b[39m=\u001b[39mlimits, user_api\u001b[39m=\u001b[39muser_api)\n\u001b[0;32m    138\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 139\u001b[0m     \u001b[39mreturn\u001b[39;00m threadpoolctl\u001b[39m.\u001b[39;49mthreadpool_limits(limits\u001b[39m=\u001b[39;49mlimits, user_api\u001b[39m=\u001b[39;49muser_api)\n",
      "File \u001b[1;32mc:\\Users\\MaxB2\\anaconda3\\lib\\site-packages\\threadpoolctl.py:171\u001b[0m, in \u001b[0;36mthreadpool_limits.__init__\u001b[1;34m(self, limits, user_api)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, limits\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, user_api\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    168\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_limits, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_user_api, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prefixes \u001b[39m=\u001b[39m \\\n\u001b[0;32m    169\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params(limits, user_api)\n\u001b[1;32m--> 171\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_threadpool_limits()\n",
      "File \u001b[1;32mc:\\Users\\MaxB2\\anaconda3\\lib\\site-packages\\threadpoolctl.py:268\u001b[0m, in \u001b[0;36mthreadpool_limits._set_threadpool_limits\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_limits \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    266\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 268\u001b[0m modules \u001b[39m=\u001b[39m _ThreadpoolInfo(prefixes\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prefixes,\n\u001b[0;32m    269\u001b[0m                           user_api\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_user_api)\n\u001b[0;32m    270\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m    271\u001b[0m     \u001b[39m# self._limits is a dict {key: num_threads} where key is either\u001b[39;00m\n\u001b[0;32m    272\u001b[0m     \u001b[39m# a prefix or a user_api. If a module matches both, the limit\u001b[39;00m\n\u001b[0;32m    273\u001b[0m     \u001b[39m# corresponding to the prefix is chosed.\u001b[39;00m\n\u001b[0;32m    274\u001b[0m     \u001b[39mif\u001b[39;00m module\u001b[39m.\u001b[39mprefix \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_limits:\n",
      "File \u001b[1;32mc:\\Users\\MaxB2\\anaconda3\\lib\\site-packages\\threadpoolctl.py:340\u001b[0m, in \u001b[0;36m_ThreadpoolInfo.__init__\u001b[1;34m(self, user_api, prefixes, modules)\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muser_api \u001b[39m=\u001b[39m [] \u001b[39mif\u001b[39;00m user_api \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m user_api\n\u001b[0;32m    339\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodules \u001b[39m=\u001b[39m []\n\u001b[1;32m--> 340\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_modules()\n\u001b[0;32m    341\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warn_if_incompatible_openmp()\n\u001b[0;32m    342\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\MaxB2\\anaconda3\\lib\\site-packages\\threadpoolctl.py:373\u001b[0m, in \u001b[0;36m_ThreadpoolInfo._load_modules\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    371\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_find_modules_with_dyld()\n\u001b[0;32m    372\u001b[0m \u001b[39melif\u001b[39;00m sys\u001b[39m.\u001b[39mplatform \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mwin32\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 373\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_find_modules_with_enum_process_module_ex()\n\u001b[0;32m    374\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    375\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_find_modules_with_dl_iterate_phdr()\n",
      "File \u001b[1;32mc:\\Users\\MaxB2\\anaconda3\\lib\\site-packages\\threadpoolctl.py:461\u001b[0m, in \u001b[0;36m_ThreadpoolInfo._find_modules_with_enum_process_module_ex\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    459\u001b[0m buf \u001b[39m=\u001b[39m (HMODULE \u001b[39m*\u001b[39m buf_count)()\n\u001b[0;32m    460\u001b[0m buf_size \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39msizeof(buf)\n\u001b[1;32m--> 461\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ps_api\u001b[39m.\u001b[39;49mEnumProcessModulesEx(\n\u001b[0;32m    462\u001b[0m         h_process, ctypes\u001b[39m.\u001b[39;49mbyref(buf), buf_size,\n\u001b[0;32m    463\u001b[0m         ctypes\u001b[39m.\u001b[39;49mbyref(needed), LIST_MODULES_ALL):\n\u001b[0;32m    464\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mEnumProcessModulesEx failed\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    465\u001b[0m \u001b[39mif\u001b[39;00m buf_size \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m needed\u001b[39m.\u001b[39mvalue:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "estimators = [etc,rfr,knn]\n",
    "data4blender = pd.DataFrame(columns=['data', 'label'])\n",
    "def fill_dataframe(data, labels):\n",
    "    # Check if the lengths of data and labels match\n",
    "    if len(data) != len(labels):\n",
    "        raise ValueError(\"Lengths of data and labels must match.\")\n",
    "    # Add data and labels to the DataFrame\n",
    "    data4blender['data'] = data\n",
    "    data4blender['label'] = labels\n",
    "\n",
    "\n",
    "\n",
    "triple_predictions = []\n",
    "temp_triple = []\n",
    "true_labels = y_train_50k.copy()\n",
    "\n",
    "for index in range(len(X_train_50k)):\n",
    "    for estimator in estimators:\n",
    "        temp_triple.append(estimator.predict(X_train_50k.iloc[index:index+1]))\n",
    "    triple_predictions.append([temp_triple])\n",
    "    temp_triple = []\n",
    "\n",
    "triple_predictions\n",
    "\"\"\"\n",
    "def triple_pred_plus_label(X_set,y_set,index):\n",
    "    pred1 = etc.predict(X_set.iloc[index])\n",
    "    pred2 = rfr.predict(X_set.iloc[index])\n",
    "    pred3 = knn.predict(X_set.iloc[index])\n",
    "    return (pred1,pred2,pred3),y_set[index]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9755"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Voting\n",
    "\"\"\"from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "vtc = VotingClassifier(\n",
    "    estimators=[(\"knn\",knn),(\"rfr\",rfr),(\"etc\",etc)],\n",
    "    voting=\"soft\"\n",
    ")\n",
    "\n",
    "vtc.fit(X_train_50k,y_train_50k)\n",
    "y_val_pred_vtc = vtc.predict(X_validate_10k)\n",
    "acc_y_val_pred_vtc = accuracy_score(y_validate_10k, y_val_pred_vtc)\n",
    "acc_y_val_pred_vtc\"\"\"\n",
    "\"\"\"y_test_pred_knn = knn.predict(X_test)\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc_y_test_pred_knn = accuracy_score(y_test, y_test_pred_knn)\n",
    "print(\"KNN Test Set Acc: \",acc_y_test_pred_knn)\n",
    "\n",
    "y_test_pred_vtc = vtc.predict(X_test)\n",
    "acc_y_test_pred_vtc = accuracy_score(y_test, y_test_pred_vtc)\n",
    "print(\"Voting Classifier Test Set Acc: \",acc_y_test_pred_vtc)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
