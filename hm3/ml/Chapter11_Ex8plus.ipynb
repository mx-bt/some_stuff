{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 11. Training Deep Neural Networks\n",
    "## Exercises\n",
    "1. Glorot an He initialization try to tackle the problem of exploding/vanishing gradients. Rather than just initializing the weights completely random, this mechanism ensures that the standard derivation of the output equals roughly the inputs'. Unfortunately this only works for shallow networks, respectively for a few layers.\n",
    "2. No, since the symmetry must be broken. Otherwise we will not be able to find individual gradients to tweak but all are the same and thereby the    model will not converge.\n",
    "3. Yes, this would be okay.\n",
    "4.  sigmoid: output layer of binary classification\n",
    "    softmax: output layer of multilabel classification\n",
    "    no activation function: regression\n",
    "5.  0.99999 means that the momentum is preserved at almost 100% which results in a cost function that can jump out of local minima with ease but will also bounce quite heavily around the global optima until it converges.\n",
    "6.  \n",
    "7.  Yes, it slows down training considerably because the connections build up slower (slower convergence). It does not slow down inference however. MC dropout will slow down inference only. It requires multiple predictions with dropped out neurons each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Practice training a deep neural network on the CIFAR10 image dataset:\n",
    "- Build a DNN with 20 hidden layers of 100 neurons each (that’s too\n",
    "many, but it’s the point of this exercise). Use He initialization and\n",
    "the Swish activation function.\n",
    "- Using Nadam optimization and early stopping, train the network on\n",
    "the CIFAR10 dataset. You can load it with\n",
    "tf.keras.datasets.cifar10.load_ data(). The dataset is\n",
    "composed of 60,000 32 × 32–pixel color images (50,000 for\n",
    "training, 10,000 for testing) with 10 classes, so you’ll need a\n",
    "softmax output layer with 10 neurons. Remember to search for the\n",
    "right learning rate each time you change the model’s architecture or\n",
    "hyperparameters.\n",
    "- Now try adding batch normalization and compare the learning\n",
    "curves: is it converging faster than before? Does it produce a\n",
    "better model? How does it affect training speed?\n",
    "- Try replacing batch normalization with SELU, and make the\n",
    "necessary adjustments to ensure the network self-normalizes (i.e.,\n",
    "standardize the input features, use LeCun normal initialization,\n",
    "make sure the DNN contains only a sequence of dense layers, etc.).\n",
    "- Try regularizing the model with alpha dropout. Then, without\n",
    "retraining your model, see if you can achieve better accuracy using\n",
    "MC dropout.\n",
    "- Retrain your model using 1cycle scheduling and see if it improves\n",
    "training speed and model accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "5623/5625 [============================>.] - ETA: 0s - loss: 2.4017 - accuracy: 0.1029INFO:tensorflow:Assets written to: my_cifar10_model\\assets\n",
      "5625/5625 [==============================] - 25s 4ms/step - loss: 2.4017 - accuracy: 0.1029 - val_loss: 2.3118 - val_accuracy: 0.1064\n",
      "Epoch 2/100\n",
      "5614/5625 [============================>.] - ETA: 0s - loss: 2.3245 - accuracy: 0.0991INFO:tensorflow:Assets written to: my_cifar10_model\\assets\n",
      "5625/5625 [==============================] - 25s 4ms/step - loss: 2.3245 - accuracy: 0.0992 - val_loss: 2.3100 - val_accuracy: 0.1024\n",
      "Epoch 3/100\n",
      "5617/5625 [============================>.] - ETA: 0s - loss: 2.4139 - accuracy: 0.0999INFO:tensorflow:Assets written to: my_cifar10_model\\assets\n",
      "5625/5625 [==============================] - 27s 5ms/step - loss: 2.4137 - accuracy: 0.0999 - val_loss: 2.3068 - val_accuracy: 0.0976\n",
      "Epoch 4/100\n",
      "5625/5625 [==============================] - 26s 5ms/step - loss: 2.3188 - accuracy: 0.1012 - val_loss: 2.3137 - val_accuracy: 0.0950\n",
      "Epoch 5/100\n",
      "5613/5625 [============================>.] - ETA: 0s - loss: 2.3093 - accuracy: 0.0985INFO:tensorflow:Assets written to: my_cifar10_model\\assets\n",
      "5625/5625 [==============================] - 28s 5ms/step - loss: 2.3093 - accuracy: 0.0984 - val_loss: 2.3035 - val_accuracy: 0.0986\n",
      "Epoch 6/100\n",
      "5625/5625 [==============================] - 24s 4ms/step - loss: 2.3064 - accuracy: 0.1016 - val_loss: 2.3046 - val_accuracy: 0.1038\n",
      "Epoch 7/100\n",
      "5625/5625 [==============================] - 24s 4ms/step - loss: 2.3066 - accuracy: 0.0990 - val_loss: 2.3072 - val_accuracy: 0.0970\n",
      "Epoch 8/100\n",
      "5625/5625 [==============================] - 25s 5ms/step - loss: 2.3064 - accuracy: 0.0985 - val_loss: 2.3351 - val_accuracy: 0.1038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'import pandas as pd\\nimport matplotlib.pyplot as plt\\nnum_epochs_used = early_stopping_cb.stopped_epoch + 1\\npd.DataFrame(history.history).plot(figsize=(8, 5), xlim=[0, num_epochs_used], ylim=[0, 1], grid=True, xlabel=\"Epoch\",style=[\"r--\", \"r--.\", \"b-\", \"b-*\"])\\nplt.show()'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from joblib import load\n",
    "cifar10 = load(\"C:/Users/MaxB2/Documents/Machine_Is_Learning/cifar10_data.joblib\")\n",
    "\n",
    "X_train, y_train = cifar10[0][0]/255.,cifar10[0][1]\n",
    "X_test, y_test = cifar10[1][0]/255.,cifar10[1][1]\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.keras.backend.clear_session()\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=[32, 32, 3]),\n",
    "    tf.keras.layers.Dense(100,activation=\"swish\",kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(100,activation=\"swish\",kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(100,activation=\"swish\",kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(100,activation=\"swish\",kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dense(100,activation=\"swish\",kernel_initializer=\"he_normal\"),  # 5\n",
    "    tf.keras.layers.Dense(10,activation=\"softmax\")\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=5e-3)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping_cb = EarlyStopping(patience=3, monitor='val_loss', restore_best_weights=True)\n",
    "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"my_cifar10_model\", save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=100,\n",
    "                    batch_size=8,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[early_stopping_cb,checkpoint_cb])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "num_epochs_used = early_stopping_cb.stopped_epoch + 1\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5), xlim=[0, num_epochs_used], ylim=[0, 1], grid=True, xlabel=\"Epoch\",style=[\"r--\", \"r--.\", \"b-\", \"b-*\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Nadam(learning_rate=1e-3)  # tune learning rate, compile and fit otra vez\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=1000,\n",
    "                    validation_split=0.1,\n",
    "                    callbacks=[early_stopping_cb,checkpoint_cb])\n",
    "\n",
    "\n",
    "num_epochs_used = early_stopping_cb.stopped_epoch + 1\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5), xlim=[0, num_epochs_used], ylim=[0, 1], grid=True, xlabel=\"Epoch\",style=[\"r--\", \"r--.\", \"b-\", \"b-*\"])\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
